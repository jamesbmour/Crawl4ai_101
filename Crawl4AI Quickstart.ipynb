{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0cba38e5",
      "metadata": {
        "id": "0cba38e5"
      },
      "source": [
        "#  üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scrapper\n",
        "<div align=\"center\">\n",
        "\n",
        "<a href=\"https://trendshift.io/repositories/11716\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/11716\" alt=\"unclecode%2Fcrawl4ai | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n",
        "\n",
        "[![GitHub Stars](https://img.shields.io/github/stars/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/stargazers)\n",
        "[![GitHub Forks](https://img.shields.io/github/forks/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/network/members)\n",
        "\n",
        "[![PyPI version](https://badge.fury.io/py/crawl4ai.svg)](https://badge.fury.io/py/crawl4ai)\n",
        "[![Python Version](https://img.shields.io/pypi/pyversions/crawl4ai)](https://pypi.org/project/crawl4ai/)\n",
        "[![Downloads](https://static.pepy.tech/badge/crawl4ai/month)](https://pepy.tech/project/crawl4ai)\n",
        "\n",
        "[![License](https://img.shields.io/github/license/unclecode/crawl4ai)](https://github.com/unclecode/crawl4ai/blob/main/LICENSE)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Security: bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)\n",
        "\n",
        "</div>\n",
        "\n",
        "Crawl4AI simplifies asynchronous web crawling and data extraction, making it accessible for large language models (LLMs) and AI applications. üÜìüåê\n",
        "\n",
        "- GitHub Repository: [https://github.com/unclecode/crawl4ai](https://github.com/unclecode/crawl4ai)\n",
        "- Twitter: [@unclecode](https://twitter.com/unclecode)\n",
        "- Website: [https://crawl4ai.com](https://crawl4ai.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41de6458",
      "metadata": {
        "id": "41de6458"
      },
      "source": [
        "### **Quickstart with Crawl4AI**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1380e951",
      "metadata": {
        "id": "1380e951"
      },
      "source": [
        "#### 1. **Installation**\n",
        "Install Crawl4AI and necessary dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "05fecfad",
      "metadata": {
        "id": "05fecfad"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# !pip install -U crawl4ai\n",
        "# !pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ZSFh3BtZdfb6",
      "metadata": {
        "id": "ZSFh3BtZdfb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4.247\n"
          ]
        }
      ],
      "source": [
        "# Check crawl4ai version\n",
        "import crawl4ai\n",
        "print(crawl4ai.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JJE1jyrol8hn",
      "metadata": {
        "id": "JJE1jyrol8hn"
      },
      "source": [
        "##### Setup Crawl4ai\n",
        "The following command installs Playride and its dependencies and updates a few configurations for Crawl4ai. After that, you can run the doctor command to ensure everything works as it should.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "31TTKe7AhhpK",
      "metadata": {
        "collapsed": true,
        "id": "31TTKe7AhhpK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# !crawl4ai-setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "fpxycFlXYJOb",
      "metadata": {
        "id": "fpxycFlXYJOb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INIT].... ‚Üí Running Crawl4AI health check...\n",
            "[INIT].... ‚Üí Crawl4AI 0.4.247\n",
            "[TEST].... ‚Ñπ Testing crawling capabilities...\n",
            "[EXPORT].. ‚Ñπ Exporting PDF and taking screenshot took 0.57s\n",
            "[FETCH]... ‚Üì https://crawl4ai.com... | Status: True | Time: 1.69s\n",
            "[SCRAPE].. ‚óÜ Processed https://crawl4ai.com... | Time: 12ms\n",
            "[COMPLETE] ‚óè https://crawl4ai.com... | Status: True | Total: 1.70s\n",
            "[COMPLETE] ‚óè ‚úÖ Crawling test passed!\n"
          ]
        }
      ],
      "source": [
        "# !crawl4ai-doctor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "i-uP1YRMYRpC",
      "metadata": {
        "id": "i-uP1YRMYRpC"
      },
      "outputs": [],
      "source": [
        "# If you face with an error try it manually\n",
        "# !playwright install --with-deps chrome # Recommended for Colab/Linux"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Af4BY4Zvlf0D",
      "metadata": {
        "id": "Af4BY4Zvlf0D"
      },
      "source": [
        "I suggest you first try the code below to ensure that Playwright is installed and works properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2c2a74c8",
      "metadata": {
        "id": "2c2a74c8"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "by3AVeSwlcba",
      "metadata": {
        "id": "by3AVeSwlcba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Example Domain\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def test_browser():\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        await page.goto('https://example.com')\n",
        "        print(f'Title: {await page.title()}')\n",
        "        await browser.close()\n",
        "\n",
        "asyncio.run(test_browser())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a5a61540",
      "metadata": {},
      "outputs": [],
      "source": [
        "!playwright install"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3c558d7",
      "metadata": {
        "id": "f3c558d7"
      },
      "source": [
        "#### 2. **Basic Setup and Simple Crawl**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "003376f3",
      "metadata": {
        "id": "003376f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.247\n",
            "[FETCH]... ‚Üì https://www.kidocode.com/degrees/technology... | Status: True | Time: 0.86s\n",
            "[SCRAPE].. ‚óÜ Processed https://www.kidocode.com/degrees/technology... | Time: 24ms\n",
            "[COMPLETE] ‚óè https://www.kidocode.com/degrees/technology... | Status: True | Total: 0.89s\n",
            "[![coding school for kids](https://cdn.prod.website-files.com/61d6943d6b5924685ac825ca/64a6a12136e8f756c9df3baa_k-combomark-white.svg)](https://www.kidocode.com/degrees/</>) -- [Trial Class](https://www.kidocode.com/degrees/</trial-class>) -- Degrees -- degrees -- [All Degrees](https://www.kidocode.com/degrees/</degrees>) -- [AI Degree](https://www.kidocode.com/degrees/</degrees/artificial-intelligence>) -- [Technology Degree](https://www.kidocode.com/degrees/</degrees/technology>) -- [Entrepreneurship Degree](https\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from crawl4ai import AsyncWebCrawler, CacheMode, BrowserConfig, CrawlerRunConfig, CacheMode\n",
        "\n",
        "async def simple_crawl():\n",
        "    crawler_run_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS)\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.kidocode.com/degrees/technology\",\n",
        "            config=crawler_run_config\n",
        "        )\n",
        "        print(result.markdown_v2.raw_markdown[:500].replace(\"\\n\", \" -- \"))  # Print the first 500 characters\n",
        "\n",
        "asyncio.run(simple_crawl())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da9b4d50",
      "metadata": {
        "id": "da9b4d50"
      },
      "source": [
        "#### 3. **Dynamic Content Handling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5bb8c1e4",
      "metadata": {
        "id": "5bb8c1e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.247\n",
            "[FETCH]... ‚Üì https://www.nbcnews.com/business... | Status: True | Time: 0.05s\n",
            "[COMPLETE] ‚óè https://www.nbcnews.com/business... | Status: True | Total: 0.06s\n",
            "IE 11 is not supported. For an optimal experience visit our site on another browser. -- Skip to Content -- [NBC News Logo](https://www.nbcnews.com/<https:/www.nbcnews.com>) -- Sponsored By --   * [LIVE: Wildfires](https://www.nbcnews.com/<https:/www.nbcnews.com/weather/wildfires/live-blog/california-wildfires-live-updates-santa-ana-winds-continue-rcna187351>) --   * [LIVE: MNF Playoffs](https://www.nbcnews.com/<https:/www.nbcnews.com/sports/nfl/live-blog/nfl-playoffs-minnesota-vikings-la-rams-wild-card-weekend\n"
          ]
        }
      ],
      "source": [
        "async def crawl_dynamic_content():\n",
        "    # You can use wait_for to wait for a condition to be met before returning the result\n",
        "    # wait_for = \"\"\"() => {\n",
        "    #     return Array.from(document.querySelectorAll('article.tease-card')).length > 10;\n",
        "    # }\"\"\"\n",
        "\n",
        "    # wait_for can be also just a css selector\n",
        "    # wait_for = \"article.tease-card:nth-child(10)\"\n",
        "\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        js_code = [\n",
        "            \"const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();\"\n",
        "        ]\n",
        "        config = CrawlerRunConfig(\n",
        "            cache_mode=CacheMode.ENABLED,\n",
        "            js_code=js_code,\n",
        "            # wait_for=wait_for,\n",
        "        )\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business\",\n",
        "            config=config,\n",
        "\n",
        "        )\n",
        "        print(result.markdown_v2.raw_markdown[:500].replace(\"\\n\", \" -- \"))  # Print first 500 characters\n",
        "\n",
        "asyncio.run(crawl_dynamic_content())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86febd8d",
      "metadata": {
        "id": "86febd8d"
      },
      "source": [
        "#### 4. **Content Cleaning and Fit Markdown**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "8e8ab01f",
      "metadata": {
        "id": "8e8ab01f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.247\n",
            "[FETCH]... ‚Üì https://en.wikipedia.org/wiki/Apple... | Status: True | Time: 0.02s\n",
            "[COMPLETE] ‚óè https://en.wikipedia.org/wiki/Apple... | Status: True | Total: 0.03s\n",
            "Full Markdown Length: 89097\n",
            "Fit Markdown Length: 72964\n"
          ]
        }
      ],
      "source": [
        "from crawl4ai.content_filter_strategy import PruningContentFilter\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "async def clean_content():\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        config = CrawlerRunConfig(\n",
        "            cache_mode=CacheMode.ENABLED,\n",
        "            excluded_tags=['nav', 'footer', 'aside'],\n",
        "            remove_overlay_elements=True,\n",
        "            markdown_generator=DefaultMarkdownGenerator(\n",
        "                content_filter=PruningContentFilter(threshold=0.48, threshold_type=\"fixed\", min_word_threshold=0),\n",
        "                options={\n",
        "                    \"ignore_links\": True\n",
        "                }\n",
        "            ),\n",
        "        )\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://en.wikipedia.org/wiki/Apple\",\n",
        "            config=config,\n",
        "        )\n",
        "        full_markdown_length = len(result.markdown_v2.raw_markdown)\n",
        "        fit_markdown_length = len(result.markdown_v2.fit_markdown)\n",
        "        print(f\"Full Markdown Length: {full_markdown_length}\")\n",
        "        print(f\"Fit Markdown Length: {fit_markdown_length}\")\n",
        "\n",
        "\n",
        "asyncio.run(clean_content())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55715146",
      "metadata": {
        "id": "55715146"
      },
      "source": [
        "#### 5. **Link Analysis and Smart Filtering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "2ae47c69",
      "metadata": {
        "id": "2ae47c69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.247\n",
            "[FETCH]... ‚Üì https://www.nbcnews.com/business... | Status: True | Time: 0.04s\n",
            "[COMPLETE] ‚óè https://www.nbcnews.com/business... | Status: True | Total: 0.05s\n",
            "Found 121 internal links\n",
            "Found 32 external links\n",
            "Href: https://www.nbcnews.com\n",
            "Text: NBC News Logo\n",
            "\n",
            "Href: https://www.nbcnews.com/weather/wildfires/live-blog/california-wildfires-live-updates-santa-ana-winds-continue-rcna187351\n",
            "Text: LIVE: Wildfires\n",
            "\n",
            "Href: https://www.nbcnews.com/sports/nfl/live-blog/nfl-playoffs-minnesota-vikings-la-rams-wild-card-weekend-live-updates-rcna187233\n",
            "Text: LIVE: MNF Playoffs\n",
            "\n",
            "Href: https://www.nbcnews.com/new-york\n",
            "Text: New York\n",
            "\n",
            "Href: https://www.nbcnews.com/los-angeles\n",
            "Text: Los Angeles\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "async def link_analysis():\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        config = CrawlerRunConfig(\n",
        "            cache_mode=CacheMode.ENABLED,\n",
        "            exclude_external_links=True,\n",
        "            exclude_social_media_links=True,\n",
        "            # exclude_domains=[\"facebook.com\", \"twitter.com\"]\n",
        "        )\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business\",\n",
        "            config=config,\n",
        "        )\n",
        "        print(f\"Found {len(result.links['internal'])} internal links\")\n",
        "        print(f\"Found {len(result.links['external'])} external links\")\n",
        "\n",
        "        for link in result.links['internal'][:5]:\n",
        "            print(f\"Href: {link['href']}\\nText: {link['text']}\\n\")\n",
        "\n",
        "\n",
        "asyncio.run(link_analysis())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80cceef3",
      "metadata": {
        "id": "80cceef3"
      },
      "source": [
        "#### 6. **Media Handling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "1fed7f99",
      "metadata": {
        "id": "1fed7f99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.247\n",
            "[FETCH]... ‚Üì https://www.nbcnews.com/business... | Status: True | Time: 0.04s\n",
            "[COMPLETE] ‚óè https://www.nbcnews.com/business... | Status: True | Total: 0.04s\n",
            "Image URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-762x508,f_auto,q_auto:best/rockcms/2025-01/250114-Mark-Zuckerberg-ch-1357-1e597b.jpg, Alt: Mark Zuckerberg., Score: 6\n",
            "Image URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-762x508,f_auto,q_auto:best/rockcms/2025-01/250114-boars-head-vl-145p-5d0cc8.jpg, Alt: Boar's Head meat varieties are seen displayed at a grocery store, Score: 6\n",
            "Image URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-80x80,f_auto,q_auto:best/rockcms/2025-01/250114-capitol-one-bank-vl-1149a-dfe41d.jpg, Alt: People walk past a Capital One bank, Score: 6\n",
            "Image URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-80x80,f_auto,q_auto:best/rockcms/2025-01/250114-nurse-stock-mn-1130-efc12b.jpg, Alt: Nurse with arms crossed, Score: 6\n",
            "Image URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-80x80,f_auto,q_auto:best/rockcms/2025-01/250114-southwest-airlines-ch-1119-b859ea.jpg, Alt: A Southwest Airlines aircraft., Score: 6\n"
          ]
        }
      ],
      "source": [
        "async def media_handling():\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        config = CrawlerRunConfig(\n",
        "            cache_mode=CacheMode.ENABLED,\n",
        "            exclude_external_images=False,\n",
        "            # screenshot=True # Set this to True if you want to take a screenshot\n",
        "        )\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business\",\n",
        "            config=config,\n",
        "        )\n",
        "        for img in result.media['images'][:5]:\n",
        "            print(f\"Image URL: {img['src']}, Alt: {img['alt']}, Score: {img['score']}\")\n",
        "\n",
        "asyncio.run(media_handling())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9290499a",
      "metadata": {
        "id": "9290499a"
      },
      "source": [
        "#### 7. **Using Hooks for Custom Workflow**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d069c2b",
      "metadata": {
        "id": "9d069c2b"
      },
      "source": [
        "Hooks in Crawl4AI allow you to run custom logic at specific stages of the crawling process. This can be invaluable for scenarios like setting custom headers, logging activities, or processing content before it is returned. Below is an example of a basic workflow using a hook, followed by a complete list of available hooks and explanations on their usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "bc4d2fc8",
      "metadata": {
        "id": "bc4d2fc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.247\n",
            "[HOOK] before_goto - About to visit: https://crawl4ai.com\n",
            "[FETCH]... ‚Üì https://crawl4ai.com... | Status: True | Time: 1.17s\n",
            "[SCRAPE].. ‚óÜ Processed https://crawl4ai.com... | Time: 19ms\n",
            "[COMPLETE] ‚óè https://crawl4ai.com... | Status: True | Total: 1.20s\n",
            "[Crawl4AI Documentation](https://crawl4ai.com/<https:/docs.crawl4ai.com/>) --   * [ Home ](https://crawl4ai.com/<.>) --   * [ Quick Start ](https://crawl4ai.com/<core/quickstart/>) --   * [ Search ](https://crawl4ai.com/<#>) --  --  --   * Home --   * Setup & Installation --     * [Installation](https://crawl4ai.com/<core/installation/>) --     * [Docker Deployment](https://crawl4ai.com/<core/docker-deploymeny/>) --   * [Quick Start](https://crawl4ai.com/<core/quickstart/>) --   * Blog & Changelog --     * [Blog Home](https://craw\n"
          ]
        }
      ],
      "source": [
        "from playwright.async_api import Page, BrowserContext\n",
        "\n",
        "async def before_goto(page: Page, context: BrowserContext, url: str, **kwargs):\n",
        "        \"\"\"Hook called before navigating to each URL\"\"\"\n",
        "        print(f\"[HOOK] before_goto - About to visit: {url}\")\n",
        "        # Example: Add custom headers for the request\n",
        "        await page.set_extra_http_headers({\n",
        "            \"Custom-Header\": \"my-value\"\n",
        "        })\n",
        "        return page\n",
        "\n",
        "async def custom_hook_workflow(verbose=True):\n",
        "    async with AsyncWebCrawler(config=BrowserConfig( verbose=verbose)) as crawler:\n",
        "        # Set a 'before_goto' hook to run custom code just before navigation\n",
        "        crawler.crawler_strategy.set_hook(\"before_goto\", before_goto)\n",
        "\n",
        "        # Perform the crawl operation\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://crawl4ai.com\",\n",
        "            config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n",
        "        )\n",
        "        print(result.markdown_v2.raw_markdown[:500].replace(\"\\n\", \" -- \"))\n",
        "\n",
        "asyncio.run(custom_hook_workflow())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff45e21",
      "metadata": {
        "id": "3ff45e21"
      },
      "source": [
        "List of available hooks and examples for each stage of the crawling process:\n",
        "\n",
        "- **on_browser_created**\n",
        "    ```python\n",
        "    async def on_browser_created_hook(browser):\n",
        "        print(\"[Hook] Browser created\")\n",
        "    ```\n",
        "\n",
        "- **before_goto**\n",
        "    ```python\n",
        "    async def before_goto_hook(page, context = None):\n",
        "        await page.set_extra_http_headers({\"X-Test-Header\": \"test\"})\n",
        "    ```\n",
        "\n",
        "- **after_goto**\n",
        "    ```python\n",
        "    async def after_goto_hook(page, context = None):\n",
        "        print(f\"[Hook] Navigated to {page.url}\")\n",
        "    ```\n",
        "\n",
        "- **on_execution_started**\n",
        "    ```python\n",
        "    async def on_execution_started_hook(page, context = None):\n",
        "        print(\"[Hook] JavaScript execution started\")\n",
        "    ```\n",
        "\n",
        "- **before_return_html**\n",
        "    ```python\n",
        "    async def before_return_html_hook(page, html, context = None):\n",
        "        print(f\"[Hook] HTML length: {len(html)}\")\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d56ebb1",
      "metadata": {
        "id": "2d56ebb1"
      },
      "source": [
        "#### 8. **Session-Based Crawling**\n",
        "\n",
        "When to Use Session-Based Crawling:\n",
        "Session-based crawling is especially beneficial when navigating through multi-page content where each page load needs to maintain the same session context. For instance, in cases where a ‚ÄúNext Page‚Äù button must be clicked to load subsequent data, the new data often replaces the previous content. Here, session-based crawling keeps the browser state intact across each interaction, allowing for sequential actions within the same session. An easy way to think about a session is that it acts like a browser tab; when you pass the same session ID, it uses the same browser tab and does not create a new tab.\n",
        "\n",
        "Example: Multi-Page Navigation Using JavaScript\n",
        "In this example, we‚Äôll navigate through multiple pages by clicking a \"Next Page\" button. After each page load, we extract the new content and repeat the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "e7bfebae",
      "metadata": {
        "id": "e7bfebae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Advanced Multi-Page Crawling with JavaScript Execution ---\n",
            "[INIT].... ‚Üí Crawl4AI 0.4.247\n",
            "[FETCH]... ‚Üì https://github.com/microsoft/TypeScript/commits/ma... | Status: True | Time: 1.36s\n",
            "[SCRAPE].. ‚óÜ Processed https://github.com/microsoft/TypeScript/commits/ma... | Time: 70ms\n",
            "[EXTRACT]. ‚ñ† Completed for https://github.com/microsoft/TypeScript/commits/ma... | Time: 0.05136804199719336s\n",
            "[COMPLETE] ‚óè https://github.com/microsoft/TypeScript/commits/ma... | Status: True | Total: 1.50s\n",
            "Page 1: Found 35 commits\n",
            "DOM content loaded after script execution in 0.008356809616088867\n",
            "[ERROR]... √ó Error updating image dimensions: Page.evaluate: Execution context was destroyed, most likely because of a navigation\n",
            "[FETCH]... ‚Üì https://github.com/microsoft/TypeScript/commits/ma... | Status: True | Time: 0.89s\n",
            "[SCRAPE].. ‚óÜ Processed https://github.com/microsoft/TypeScript/commits/ma... | Time: 68ms\n",
            "[EXTRACT]. ‚ñ† Completed for https://github.com/microsoft/TypeScript/commits/ma... | Time: 0.10118179199344013s\n",
            "[COMPLETE] ‚óè https://github.com/microsoft/TypeScript/commits/ma... | Status: True | Total: 1.07s\n",
            "Page 2: Found 35 commits\n",
            "[WARNING]. ‚ö† Session auto-kill is enabled in the new version. No need to manually kill sessions.\n",
            "Successfully crawled 70 commits across 3 pages\n"
          ]
        }
      ],
      "source": [
        "from crawl4ai.extraction_strategy import (\n",
        "    JsonCssExtractionStrategy,\n",
        "    LLMExtractionStrategy,\n",
        ")\n",
        "import json\n",
        "\n",
        "async def crawl_dynamic_content_pages_method_2():\n",
        "    print(\"\\n--- Advanced Multi-Page Crawling with JavaScript Execution ---\")\n",
        "\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n",
        "        session_id = \"typescript_commits_session\"\n",
        "        all_commits = []\n",
        "        last_commit = \"\"\n",
        "\n",
        "        js_next_page_and_wait = \"\"\"\n",
        "        (async () => {\n",
        "            const getCurrentCommit = () => {\n",
        "                const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n",
        "                return commits.length > 0 ? commits[0].textContent.trim() : null;\n",
        "            };\n",
        "\n",
        "            const initialCommit = getCurrentCommit();\n",
        "            const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n",
        "            if (button) button.click();\n",
        "\n",
        "            // Poll for changes\n",
        "            while (true) {\n",
        "                await new Promise(resolve => setTimeout(resolve, 100)); // Wait 100ms\n",
        "                const newCommit = getCurrentCommit();\n",
        "                if (newCommit && newCommit !== initialCommit) {\n",
        "                    break;\n",
        "                }\n",
        "            }\n",
        "        })();\n",
        "        \"\"\"\n",
        "\n",
        "        schema = {\n",
        "            \"name\": \"Commit Extractor\",\n",
        "            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n",
        "            \"fields\": [\n",
        "                {\n",
        "                    \"name\": \"title\",\n",
        "                    \"selector\": \"h4.markdown-title\",\n",
        "                    \"type\": \"text\",\n",
        "                    \"transform\": \"strip\",\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "        extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "        for page in range(2):  # Crawl 2 pages\n",
        "            config = CrawlerRunConfig(\n",
        "                cache_mode=CacheMode.BYPASS,\n",
        "                session_id=session_id,\n",
        "                css_selector=\"li.Box-sc-g0xbh4-0\",\n",
        "                extraction_strategy=extraction_strategy,\n",
        "                js_code=js_next_page_and_wait if page > 0 else None,\n",
        "                js_only=page > 0,\n",
        "            )\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "            assert result.success, f\"Failed to crawl page {page + 1}\"\n",
        "\n",
        "            commits = json.loads(result.extracted_content)\n",
        "            all_commits.extend(commits)\n",
        "\n",
        "            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n",
        "\n",
        "        await crawler.crawler_strategy.kill_session(session_id)\n",
        "        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n",
        "\n",
        "asyncio.run(crawl_dynamic_content_pages_method_2())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v6myEZ6Qhwtq",
      "metadata": {
        "id": "v6myEZ6Qhwtq"
      },
      "source": [
        "#### 9. **Using Extraction Strategies**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y1gKhtzph4NV",
      "metadata": {
        "id": "Y1gKhtzph4NV"
      },
      "source": [
        "##### Executing JavaScript & Extract Structured Data without LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "QZk5kW6rhzTO",
      "metadata": {
        "id": "QZk5kW6rhzTO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.247\n",
            "DOM content loaded after script execution in 0.008465051651000977\n",
            "[FETCH]... ‚Üì https://www.kidocode.com/degrees/technology... | Status: True | Time: 0.98s\n",
            "[SCRAPE].. ‚óÜ Processed https://www.kidocode.com/degrees/technology... | Time: 29ms\n",
            "[EXTRACT]. ‚ñ† Completed for https://www.kidocode.com/degrees/technology... | Time: 0.013162999995984137s\n",
            "[COMPLETE] ‚óè https://www.kidocode.com/degrees/technology... | Status: True | Total: 1.04s\n",
            "Successfully extracted 0 companies\n",
            "No companies extracted.\n"
          ]
        }
      ],
      "source": [
        "from crawl4ai.extraction_strategy import (\n",
        "    JsonCssExtractionStrategy,\n",
        "    LLMExtractionStrategy,\n",
        ")\n",
        "import json\n",
        "async def extract():\n",
        "    schema = {\n",
        "    \"name\": \"KidoCode Courses\",\n",
        "    \"baseSelector\": \"section.charge-methodology .w-tab-content > div\",\n",
        "    \"fields\": [\n",
        "        {\n",
        "            \"name\": \"section_title\",\n",
        "            \"selector\": \"h3.heading-50\",\n",
        "            \"type\": \"text\",\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"section_description\",\n",
        "            \"selector\": \".charge-content\",\n",
        "            \"type\": \"text\",\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"course_name\",\n",
        "            \"selector\": \".text-block-93\",\n",
        "            \"type\": \"text\",\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"course_description\",\n",
        "            \"selector\": \".course-content-text\",\n",
        "            \"type\": \"text\",\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"course_icon\",\n",
        "            \"selector\": \".image-92\",\n",
        "            \"type\": \"attribute\",\n",
        "            \"attribute\": \"src\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "\n",
        "        # Create the JavaScript that handles clicking multiple times\n",
        "        js_click_tabs = \"\"\"\n",
        "        (async () => {\n",
        "            const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");\n",
        "\n",
        "            for(let tab of tabs) {\n",
        "                // scroll to the tab\n",
        "                tab.scrollIntoView();\n",
        "                tab.click();\n",
        "                // Wait for content to load and animations to complete\n",
        "                await new Promise(r => setTimeout(r, 500));\n",
        "            }\n",
        "        })();\n",
        "        \"\"\"\n",
        "\n",
        "        config = CrawlerRunConfig(\n",
        "            cache_mode=CacheMode.BYPASS,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            js_code=[js_click_tabs],\n",
        "        )\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.kidocode.com/degrees/technology\",\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        companies = json.loads(result.extracted_content)\n",
        "        print(f\"Successfully extracted {len(companies)} companies\")\n",
        "        if companies:\n",
        "            print(json.dumps(companies[0], indent=2))\n",
        "        else:\n",
        "            print(\"No companies extracted.\")\n",
        "\n",
        "await extract()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad32a778",
      "metadata": {
        "id": "ad32a778"
      },
      "source": [
        "#####  LLM Extraction\n",
        "\n",
        "This example demonstrates how to use language model-based extraction to retrieve structured data from a pricing page on OpenAI‚Äôs site."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3011a7c5",
      "metadata": {
        "id": "3011a7c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Extracting Structured Data with openai/gpt-4o-mini ---\n",
            "[INIT].... ‚Üí Crawl4AI 0.4.247\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/m7/yjmck8kn59gc9w3kdklj2lt40000gn/T/ipykernel_69381/3212904949.py:31: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  schema=OpenAIModelFee.schema(),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FETCH]... ‚Üì https://openai.com/api/pricing/... | Status: True | Time: 1.50s\n",
            "[SCRAPE].. ‚óÜ Processed https://openai.com/api/pricing/... | Time: 72ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n",
            "/Users/james/miniconda3/envs/yt310/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
            "* 'fields' has been removed\n",
            "  warnings.warn(message, UserWarning)\n",
            "13:21:40 - LiteLLM:INFO: utils.py:2796 - \n",
            "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
            "INFO:LiteLLM:\n",
            "LiteLLM completion() model= gpt-4o-mini; provider = openai\n"
          ]
        }
      ],
      "source": [
        "from crawl4ai.extraction_strategy import LLMExtractionStrategy\n",
        "from pydantic import BaseModel, Field\n",
        "import os, json\n",
        "# from google.colab import userdata\n",
        "# os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "class OpenAIModelFee(BaseModel):\n",
        "    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n",
        "    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n",
        "    output_fee: str = Field(\n",
        "        ..., description=\"Fee for output token for the OpenAI model.\"\n",
        "    )\n",
        "\n",
        "async def extract_structured_data_using_llm(provider: str, api_token: str = None, extra_headers: dict = None):\n",
        "    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n",
        "\n",
        "    # Skip if API token is missing (for providers that require it)\n",
        "    if api_token is None and provider != \"ollama\":\n",
        "        print(f\"API token is required for {provider}. Skipping this example.\")\n",
        "        return\n",
        "\n",
        "    extra_args = {\"extra_headers\": extra_headers} if extra_headers else {}\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://openai.com/api/pricing/\",\n",
        "            word_count_threshold=1,\n",
        "            extraction_strategy=LLMExtractionStrategy(\n",
        "                provider=provider,\n",
        "                api_token=api_token,\n",
        "                schema=OpenAIModelFee.schema(),\n",
        "                extraction_type=\"schema\",\n",
        "                instruction=\"\"\"Extract all model names along with fees for input and output tokens.\"\n",
        "                \"{model_name: 'GPT-4', input_fee: 'US$10.00 / 1M tokens', output_fee: 'US$30.00 / 1M tokens'}.\"\"\",\n",
        "                **extra_args\n",
        "            ),\n",
        "            cach_mode = CacheMode.ENABLED\n",
        "        )\n",
        "        print(json.loads(result.extracted_content)[:5])\n",
        "\n",
        "# Usage:\n",
        "# await extract_structured_data_using_llm(\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\"))\n",
        "# await extract_structured_data_using_llm(\"ollama/llama3.2\")\n",
        "await extract_structured_data_using_llm(\"openai/gpt-4o-mini\", os.getenv(\"OPENAI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6532db9d",
      "metadata": {
        "id": "6532db9d"
      },
      "source": [
        "**Cosine Similarity Strategy**\n",
        "\n",
        "This strategy uses semantic clustering to extract relevant content based on contextual similarity, which is helpful when extracting related sections from a single topic.\n",
        "\n",
        "IMPORTANT: This strategy uses embedding models from HuggingFace, to have a proper response time, make sure to switch to GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ec079108",
      "metadata": {
        "id": "ec079108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.247\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 22\u001b[0m\n\u001b[1;32m     15\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m crawler\u001b[38;5;241m.\u001b[39marun(\n\u001b[1;32m     16\u001b[0m             url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.nbcnews.com/business/consumer/how-mcdonalds-e-coli-crisis-inflation-politics-reflect-american-story-rcna177156\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m             extraction_strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[1;32m     18\u001b[0m             cach_mode \u001b[38;5;241m=\u001b[39m CacheMode\u001b[38;5;241m.\u001b[39mENABLED\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mloads(result\u001b[38;5;241m.\u001b[39mextracted_content)[:\u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m---> 22\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcosine_similarity_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/yt310/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
            "File \u001b[0;32m~/miniconda3/envs/yt310/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/yt310/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
            "File \u001b[0;32m~/miniconda3/envs/yt310/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
            "Cell \u001b[0;32mIn[17], line 5\u001b[0m, in \u001b[0;36mcosine_similarity_extraction\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcosine_similarity_extraction\u001b[39m():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m AsyncWebCrawler() \u001b[38;5;28;01mas\u001b[39;00m crawler:\n\u001b[0;32m----> 5\u001b[0m         strategy \u001b[38;5;241m=\u001b[39m \u001b[43mCosineStrategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mword_count_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Maximum distance between two words\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlinkage_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Linkage method for hierarchical clustering (ward, complete, average, single)\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Number of top keywords to extract\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43msim_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Similarity threshold for clustering\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43msemantic_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMcDonald\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms economic impact, American consumer trends\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Keywords to filter the content semantically using embeddings\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m crawler\u001b[38;5;241m.\u001b[39marun(\n\u001b[1;32m     16\u001b[0m             url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.nbcnews.com/business/consumer/how-mcdonalds-e-coli-crisis-inflation-politics-reflect-american-story-rcna177156\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m             extraction_strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[1;32m     18\u001b[0m             cach_mode \u001b[38;5;241m=\u001b[39m CacheMode\u001b[38;5;241m.\u001b[39mENABLED\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mloads(result\u001b[38;5;241m.\u001b[39mextracted_content)[:\u001b[38;5;241m5\u001b[39m])\n",
            "File \u001b[0;32m~/miniconda3/envs/yt310/lib/python3.10/site-packages/crawl4ai/extraction_strategy.py:402\u001b[0m, in \u001b[0;36mCosineStrategy.__init__\u001b[0;34m(self, semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, sim_threshold, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_embedding_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# import torch\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# self.device = torch.device('cpu')\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_batch_size \u001b[38;5;241m=\u001b[39m calculate_batch_size(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
            "File \u001b[0;32m~/miniconda3/envs/yt310/lib/python3.10/site-packages/crawl4ai/model_loader.py:43\u001b[0m, in \u001b[0;36mget_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_device\u001b[39m():\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     45\u001b[0m         device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "from crawl4ai.extraction_strategy import CosineStrategy\n",
        "\n",
        "async def cosine_similarity_extraction():\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        strategy = CosineStrategy(\n",
        "            word_count_threshold=10,\n",
        "            max_dist=0.2, # Maximum distance between two words\n",
        "            linkage_method=\"ward\", # Linkage method for hierarchical clustering (ward, complete, average, single)\n",
        "            top_k=3, # Number of top keywords to extract\n",
        "            sim_threshold=0.3, # Similarity threshold for clustering\n",
        "            semantic_filter=\"McDonald's economic impact, American consumer trends\", # Keywords to filter the content semantically using embeddings\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business/consumer/how-mcdonalds-e-coli-crisis-inflation-politics-reflect-american-story-rcna177156\",\n",
        "            extraction_strategy=strategy,\n",
        "            cach_mode = CacheMode.ENABLED\n",
        "        )\n",
        "        print(json.loads(result.extracted_content)[:5])\n",
        "\n",
        "asyncio.run(cosine_similarity_extraction())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff423629",
      "metadata": {
        "id": "ff423629"
      },
      "source": [
        "#### 10. **Conclusion and Next Steps**\n",
        "\n",
        "You‚Äôve explored core features of Crawl4AI, including dynamic content handling, link analysis, and advanced extraction strategies. Visit our documentation for further details on using Crawl4AI‚Äôs extensive features.\n",
        "\n",
        "- GitHub Repository: [https://github.com/unclecode/crawl4ai](https://github.com/unclecode/crawl4ai)\n",
        "- Twitter: [@unclecode](https://twitter.com/unclecode)\n",
        "- Website: [https://crawl4ai.com](https://crawl4ai.com)\n",
        "\n",
        "Happy Crawling with Crawl4AI! üï∑Ô∏èü§ñ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d34c1d35",
      "metadata": {
        "id": "d34c1d35"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "yt310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
